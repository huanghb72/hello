{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7829849-6d40-4b10-8da6-8fb65a055008",
   "metadata": {},
   "source": [
    "# Task 5b\n",
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8730902-573a-4adb-805d-9286f87c214a",
   "metadata": {},
   "source": [
    "Text embedding model to use: hkunlp/instructor-large\n",
    "https://huggingface.co/hkunlp/instructor-large\n",
    "Using the text embedding model, write a python jupyter notebook called\n",
    "cv-hotword-similarity-5b.ipynb to find similar phrases to the 3\n",
    "hot words in task 5a. Using cv-valid-dev.csv, write the Boolean (true\n",
    "for a record containing similar phrases to the hot words; false for a record\n",
    "that is not similar) into a new column called similarity. Save this\n",
    "updated file in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4e1b78-c211-4e08-bf79-9e08689c5b19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "433cfd52-20c4-453e-9b65-45147e04ea51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huang/Projects/venv/lib/python3.9/site-packages/InstructorEmbedding/instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n",
      "/Users/huang/Projects/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huang/Projects/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/huang/Projects/venv/lib/python3.9/site-packages/sentence_transformers/models/Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n"
     ]
    }
   ],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "model = INSTRUCTOR('hkunlp/instructor-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d22633-6351-4968-a37e-c5a4b7b14907",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb018548-e7e6-4d92-8cb6-66b2c968fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(sentences_a, sentences_b):\n",
    "    \"\"\" Compute cosine similarity between the input list of sentences\n",
    "    \"\"\"\n",
    "    embeddings_a = model.encode(sentences_a)\n",
    "    embeddings_b = model.encode(sentences_b)\n",
    "    similarities = cosine_similarity(embeddings_a,embeddings_b)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd68745a-b8e1-4af9-be79-7c4c7df1e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_phrases(text):\n",
    "    \"\"\" Extract single-word, 2-words, and 3-words phrases from input text\n",
    "    \"\"\"\n",
    "    # Split text to words and lowercase\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Extract all single words longer than 4 characers\n",
    "    single_words = [word for word in words if len(word)>4]\n",
    "\n",
    "    # Extract all phrases with 2 words\n",
    "    two_words = [' '.join(words[i:i+2]) for i in range(len(words)-1)]\n",
    "\n",
    "    # Extract all phrases with 3 words\n",
    "    three_words = [' '.join(words[i:i+3]) for i in range(len(words)-2)]\n",
    "\n",
    "    return single_words + two_words + three_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64df1198-6a08-4bc1-980e-a66c8b4a78e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hotwords(instruction, hotwords, text):\n",
    "    \"\"\" Detect hotwords from input text\n",
    "        Outputs:\n",
    "            - similarity score\n",
    "            - detected hotword\n",
    "            - most similar words in text\n",
    "    \"\"\" \n",
    "    # Prepare hotwords for model inputs\n",
    "    hotwords_sentences = [[instruction, hotword] for hotword in hotwords]\n",
    "\n",
    "    # Extract single,2,3-words phrases from text\n",
    "    candidates = extract_candidate_phrases(text)\n",
    "\n",
    "    # Return if the candidates list is empty\n",
    "    if not candidates:\n",
    "        return 0.0, '', ''\n",
    "        \n",
    "    # Prepare candidate phrases for model inputs\n",
    "    candidates_sentences = [[instruction, candidate] for candidate in candidates]\n",
    "\n",
    "    # Compute similarity scores\n",
    "    scores = compute_similarity(hotwords_sentences, candidates_sentences)\n",
    "\n",
    "    # Find the index of the maximum score\n",
    "    index_of_max_score = np.unravel_index(np.argmax(scores), scores.shape)\n",
    "\n",
    "    return np.max(scores), hotwords[index_of_max_score[0]], candidates[index_of_max_score[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e45c0-aac3-4fc2-a9c6-6400f2a72c51",
   "metadata": {},
   "source": [
    "# Main Processsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a952156-37f7-423f-9151-de1635431185",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = 'Represent the text for classification: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b532e50-2717-4d47-ad15-a9cba5068138",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotwords = ['be careful', 'destroy', 'stranger']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f868049-5fb4-4f34-800e-ccf6e34c52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text from csv\n",
    "csv_filename = \"cv-valid-dev.csv\"\n",
    "df = pd.read_csv(csv_filename)\n",
    "text_list = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd2a706a-e840-4ce5-a58d-2cbb6a748dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0)\t1.000 be careful [be careful]\n",
      "(3)\t0.972 be destroyed [destroy]\n",
      "(89)\t1.000 stranger [stranger]\n",
      "(395)\t0.906 danger [be careful]\n",
      "(508)\t1.000 stranger [stranger]\n",
      "(539)\t0.908 the strange [stranger]\n",
      "(674)\t1.000 stranger [stranger]\n",
      "(693)\t0.914 take care [be careful]\n",
      "(900)\t0.906 danger [be careful]\n",
      "(1036)\t0.908 the strange [stranger]\n",
      "(1067)\t0.909 need to worry [be careful]\n",
      "(1093)\t1.000 be careful [be careful]\n",
      "(1101)\t1.000 stranger [stranger]\n",
      "(1123)\t0.911 provided warnings about [be careful]\n",
      "(1243)\t1.000 stranger [stranger]\n",
      "(1311)\t0.908 the strange [stranger]\n",
      "(1445)\t0.909 need to worry [be careful]\n",
      "(1501)\t1.000 stranger [stranger]\n",
      "(1561)\t0.909 need to worry [be careful]\n",
      "(1691)\t0.906 danger [be careful]\n",
      "(1775)\t0.906 danger [be careful]\n",
      "(1781)\t0.906 danger [be careful]\n",
      "(1919)\t0.906 danger [be careful]\n",
      "(1933)\t1.000 stranger [stranger]\n",
      "(2092)\t0.923 carefully [be careful]\n",
      "(2166)\t0.915 be sure you [be careful]\n",
      "(2405)\t1.000 stranger [stranger]\n",
      "(2449)\t0.909 need to worry [be careful]\n",
      "(2453)\t0.975 strangers [stranger]\n",
      "(2685)\t0.914 take care [be careful]\n",
      "(2812)\t0.906 danger [be careful]\n",
      "(2831)\t0.906 danger [be careful]\n",
      "(2928)\t0.958 watch out [be careful]\n",
      "(2930)\t0.906 danger [be careful]\n",
      "(2994)\t0.923 carefully [be careful]\n",
      "(3045)\t0.916 the strange man [stranger]\n",
      "(3065)\t1.000 stranger [stranger]\n",
      "(3112)\t0.958 watch out [be careful]\n",
      "(3127)\t0.911 difficulties estimating risks [be careful]\n",
      "(3160)\t0.906 danger [be careful]\n",
      "(3219)\t1.000 stranger [stranger]\n",
      "(3284)\t0.947 take good care [be careful]\n",
      "(3320)\t0.906 danger [be careful]\n",
      "(3345)\t0.909 need to worry [be careful]\n",
      "(3662)\t0.916 the strange man [stranger]\n",
      "(3808)\t1.000 stranger [stranger]\n",
      "(3821)\t0.941 be safe [be careful]\n",
      "(4020)\t0.947 take good care [be careful]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the text list to detect hotwords \n",
    "detection_threshold = 0.9\n",
    "\n",
    "results = []\n",
    "for i, text in enumerate(text_list):\n",
    "    max_score, hotword, candidate = detect_hotwords(instruction, hotwords, text)\n",
    "    if max_score > detection_threshold:\n",
    "        print(f\"({i})\\t{max_score:.3f} {candidate} [{hotword}]\")\n",
    "        results.append('true')\n",
    "    else:\n",
    "        results.append('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44781aa8-9bfe-4a23-9d41-9e34f8ed01f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to the csv file\n",
    "df[\"similarity\"] = results\n",
    "df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18fe58-f10b-44ea-99a6-494318f731f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
